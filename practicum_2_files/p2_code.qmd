---
title: "Practicum 2"
format:
  pdf:
    code-block-font-size: 0.5em
editor: visual
author: "Kieran Douglas"
---
# 2. Examining Waugh’s 1927 Asparagus Data
### A.
We can see that the greatest difference between coeff estimates and those reported in the question are in the number of stalks and the variation in size (nostalks and disp). These end up being an absolute difference of 0.1767 and 0.0697 respecitvely. 

```{r}
# Set up environment and load data
library(tidyverse)
library(readr)
library(fixest)

waugh <- read_table("/Users/kieran/Documents/MASTERS/METRICS/code/metrics/practicum_2_files/waugh.txt")
waugh_clean <- waugh |> 
  rename(
    green = "GREEN",
    nostalks = "NOSTALKS",
    disp = "DISPERSE",
    price = "PRICE"
  ) |> 
   mutate(
      green = as.numeric(green),
      nostalks = as.numeric(nostalks),
      disp = as.numeric(disp),
      price = as.numeric(price)
    )

# Run a MLR
model1 = lm(price ~ green+nostalks+disp, data = waugh_clean)
summary(model1)

# compare coefficient estimates
coeffs = coefficients(model1)
original_coeffs = c(green = 0.13826, nostalks = -1.53394, disp = -0.27554)
differences <- coeffs[names(original_coeffs)] - original_coeffs
print(differences)
# We can see that the greatest difference between coeff estimates and those reported in the question are in the number of stalks and the variation in size (nostalks and disp). These end up being an absolute difference of 0.1767 and 0.0697 respecitvely. 
```

### B. 
It looks like the main issue here is that Waugh has transformed green color on the asparagus variable to inches rather than what I have in the raw data, being hundredths of inches. To fix this I need to rescale the variable green by dividing it by 100. After doing this the difference in means is no longer a problem. 
```{r}
means <- c(price_avg = mean(waugh_clean$price), green_avg = mean(waugh_clean$green), nostalks_avg = mean(waugh_clean$nostalks), disp_avg = mean(waugh_clean$disp))
print(means)
means_reported <- c(price_avg = 90.095, green_avg = 5.8875, nostalks_avg = 19.555, disp_avg = 14.875)
differences_avg <- means[names(means_reported)] - means_reported
print(differences_avg)
# It looks like the main issue here is that Waugh has transformed green color on the asparagus variable to inches rather than what I have in the raw data, being hundredths of inches. To fix this I need to rescale the variable green by dividing it by 100. 
waugh_clean <- waugh_clean |> 
  mutate(
    green = green/100
  )
# Recheck mean differences
means_recheck <- c(price_avg = mean(waugh_clean$price), green_avg = mean(waugh_clean$green), nostalks_avg = mean(waugh_clean$nostalks), disp_avg = mean(waugh_clean$disp))
print(means_recheck)

```

### C.
I notice relative variance in the relative size of the covariances reported. For example, the covariance between price and green according to Waugh is ~3430 while the one I found was 3448 (larger). On the other hand, Waugh found a covairance of ~-154 for green and disp while I found one of ~-180 (mine was smaller). In other cases though mine was larger like with the cov between green and price. I think that the pattern in differences may relate to the way my price variable is coded relative to Waugh's. This would explain the constant differences in price covariance versus the more consistent findings in ohter categories. 
```{r}
# I am now going to create a variance covariance matrix similar to Waugh's`
moments_subset <- waugh_clean[, c("price", "green", "nostalks", "disp")]
# to match the layout of the table provided I will modify green to be back to hundredths of an inch.
moments_subset <- moments_subset |> 
  mutate(
    green = green*100
  )
# Create the matric, format it correctly, and print
matrix <- cov(moments_subset)
print(matrix)
matrix_fmt <- formatC(matrix, format="f", digits=1)
matrix_fmt[lower.tri(matrix_fmt)] <- ""
print(noquote(matrix_fmt))
```

### D.
It is plausible that the principle findings concerning the effects of covariate variation could be affected in terms of the precision and statistical inference of coefficient estimates if there was underestimation or misreporting of variance and covariance among regressors. Depsite OLS estimates remaining unbiased under GM, Waugh's under or over-estiamted variance and coveriance could lead to underestiated standard errors and possibly inacurate significance. Similarly, cov among the regressors included may inflate the standard errors and lead to reductions in the validity/reliability of estimates. The difference in scaled coefficients between Waugh's and my own analysis do not differ too much, but there are some clear differences. For example, my estimate for the marginal effect of a unit increase in the number of stalks per bunch is about $0.5 less than Waugh's. I estimate a slightly higher increase in price given a unit increase in green, and a roughly $0.2 larger decrease in price given a unit increase in size variation. Overall, our estimates are very similar. I find the association between both green and nostalks with price to be highly statistically significant at the p<0.001 level and the association between disp and price to be statistically signifiacnt at **0.001<p<0.01** level. All of these observations are ceterus parabus. 

 ```{r}
  # run model
coef(model1) * 2.782
waugh_scaled <- original_coeffs*2.782
print(waugh_scaled)
 
# The difference in scaled coefficients between Waugh's and my own analysis do not differ too much, but there are some clear differences. For example, my estimate for the marginal effect of a unit increase in the number of stalks per bunch is about $0.5 less than Waugh's. I estimate a slightly higher increase in price given a unit increase in green, and a roughly $0.2 larger decrease in price given a unit increase in size variation. Overall, our estimates are very similar. I find the association between both green and nostalks with price to be highly statistically significant at the p<0.001 level and the association between disp and price to be statistically signifiacnt at 0.001<p<0.01 level. All of these observations are ceterus parabus. 
 ```

### E. 
To solve this I am going to experiment with a matrix. 
``` {r}
# my matrix solutions
# Extract covariance matrix of regressors (columns and rows 2 to 4)
xx <- matrix[2:4, 2:4]
# Extract covariance vector of PRICE with regressors (row 1, columns 2 to 4)
xy <- matrix[1, 2:4]
# Compute beta_hat = solve(Sigma_XX) %*% Sigma_XY
beta_hat <- solve(xx) %*% xy
print(beta_hat)

# Waugh's matrix solutions
vc_mat <- matrix(c(
  1063.64, 3430.89, -100.92, -82.35,
  3430.89, 24317.19, -17.01, -154.54,
  -100.92, -17.01, 61.33, 25.51,
  -82.35, -154.54, 25.51, 83.07
), nrow = 4, byrow = TRUE)
# names for reference
row_col_names <- c("price","green","nostalks","disp")
dimnames(vc_mat) <- list(row_col_names, row_col_names)
# price-cov covariance vector
Sigma_yX <- vc_mat["price", c("green", "nostalks", "disp")]
# covariate-covariance matrix
Sigma_XX <- vc_mat[c("green", "nostalks", "disp"), c("green", "nostalks", "disp")]
# raw coefficients from matrix
beta_raw <- solve(Sigma_XX, Sigma_yX)
names(beta_raw) <- c("green", "nostalks", "disperse")
# scale
beta_final <- beta_raw * 2.782
beta_final
# Computing the least squares estimates from Waugh's VC matrix table, we can see that his estimates are fairly similar in absolute value to mine. The differences in findings may come down to several factors, including that perhaps due to the nature of OLS and our estimators being assumed to be BLUE, we have better linear unbiased estimators than Waugh was able to find. It is possible that he could have faced some rounding errors here and there that I did not, both in the calculation of his var-cov matrix and his coefficients. 
```

# 3. Exploring Relationships among $R^2$, Coeﬃcients of Determination, and Correlation Coeﬃcients
### A.
Based on the cor matrix, it is evident that the most highly correlated variables are price and green, price and nostalks, nostalks and disp, and price and disp (ordered from highest to lowest positive and negative correlation). The most orthogonal correlations are between green and nostalks and green and disp becasue each of their correlations is quite close to zero, thus implying low to no real linear relationship between the variables based on the data we have available. 
```{r}
# create cor matrix
order <- c("price", "green", "nostalks", "disp")
cor_matrix <- cor(waugh_clean)
cor_matrix <- cor_matrix[order, order]

# round the matrix 
cor_matrix_rounded <- round(cor_matrix, 5)

# lower triangle with empty strings 
cor_matrix_char <- format(cor_matrix_rounded, nsmall = 5)
cor_matrix_char[lower.tri(cor_matrix_char)] <- ""

print(cor_matrix_char, quote = FALSE)
# based on the cor matrix, it is evident that the most highly correlated variables are price and green, price and nostalks, nostalks and disp, and price and disp (ordered from highest to lowest positive and negative correlation). 
```

### B.
Comparing the square roots calculated of the R^2 I can see that they are equal except for sign. This is because the correlation coefficient can be negative (between -1 and 1) but the coiefficient of determination will always be positive (between 0 and 1) since its the squared correlation coefficient. If I had run the reverse regressios the R^2 measures would be the same as those from the correct regressions because when dealing with the same two variables, the ammount of variation explained in one variable by the other will be constant since again, it is the squares correlation coefficient. Reversing their order has no impact on the ammount of variation explained that our R^2 captures. 
```{r}
# price model
pricegreenmodel <- lm(data = waugh_clean, price ~ green)
pricenostalksmodel <- lm(data = waugh_clean, price ~ nostalks)
pricedispmodel <- lm(data = waugh_clean, price ~ disp)
#price on green 
summary(pricegreenmodel)
sqrt(0.56)
# price on nostalks
summary(pricenostalksmodel)
sqrt(0.1653)
# price on disp
summary(pricedispmodel)
sqrt(0.1054)
```

### C.
I expect that if I add the regressor nostalks to the regression, the R^2 will increase. This is because we have seperately confirmed that variation in nostalks explains some of the variation in price, so if we add it to the regression, the models explanatory power should increase. Given the correlation between green and nostalks in the table I would expect the change in R^2 when nostalks is added to the regression to be large. My intuition is such that since the two variables are not super correlated BUT are indidivually correlated with price, they should explain a significant ammount of the variation in price when both included as regressors. Running the regression confirms my intuition, as the R^2 is increased by nearly 15 points, representing a significant jump. Further, comparing the R^2 between the model that regressed price on green to the model that regressed price on green AND disp, we can see another increase in the R^2 equating to about 5 points. This represents an interesting jump, despite it being smaller in magnitude to the previous change in R^2. I think this is consistent with the sample correlation between green and disp, which is bigger than that between green and nostalks but still not so big as to fail in adding additional explanation to the model. Finally, when comparing the price on nostalks model to the price on nostalks AND disp model, we also observe a jump in R^2 that is about 4 points. This is again smaller than previous jumps and can probably be explained by the higher correlation coefficient between nostalks and disp that comes out to 0.35003, an R that would suggest higher correlation between the two explanatory variables and thus a smaller increase to the R^2 when both included in a model explaining price. This is consistent with the previous findings regarding the effect of adding correlated explanatory variables to a model. I think this is why would prioritize using adjusted R^2 since that measure penalizes for adding less relavent variables. 
```{r}
green_nostalks_model <- lm(data = waugh_clean, price~green+nostalks)
summary(green_nostalks_model)
summary(pricegreenmodel)

green_disp_model <- lm(data = waugh_clean, price~green+disp)
summary(green_disp_model)
summary(pricegreenmodel)

nostalks_disp_model <- lm(data = waugh_clean, price~nostalks+disp)
summary(nostalks_disp_model)
summary(pricenostalksmodel)
```

### D.
Here I notice the same trend, where the R^2 for the full regression is 0.7268 while the summed R^2 from the single regressions is 0.8307. I think this is because in this case, the model accounts for overlaps in explanatory power between the regressors that are not even a question in the single models. I think this is becasue the multiple regression avoids double counting, providing the joint effect on price. 
```{r}
fullmodel <- lm(data = waugh_clean, price~nostalks+green+disp)
summary(fullmodel)
# R^2 of 0.7268
0.56+0.1653+0.1054
# sum of individual R^2 = 0.8307

```

### E. 
Waugh's interpretation of the coefficient of determination is incorrect. His method of adding up individual coeficients of determination is not the right way to do it, as we know the explanatory variables he is including are not orthogonal. Waugh should have first correctly calculated and second explained that the coefficient of determination is the portion of variance in price that is jointly explained by variance in the explanatory variables, and in the case of this model, it is equal to about 0.72 or 72%. 

```{r}
# nostalks = 0.14554
-1.53394 * (-100.92 / 1063.64)
# green = 0.44597
0.13826 * (3430.89 / 1063.64)
# disp = 0.02133
-0.27554 * (-82.35 / 1063.64)
```

### F.
Right off the bat I notice that the R^2 calculated are the same! I think this happened because by regressing price on a vector of fitted values that were fitted on a regression on price, I am effectively decomposing price to simply its predicted and residual parts. As for the intercept coefficient of 0 and the slope coefficient of 1, I think this is due to the fact that the vector of fitted values is a linear transformation of the original regressors, meaning that when I go onto regress price on that vector, the best linear fit for it would be a zero intercept and a 1 slope. This is because the fitted values generated are the closest possible linear predictors of the observed data. 

```{r}
fullmodel <- lm(data = waugh_clean, price~nostalks+green+disp)
summary(fullmodel)
fitted <- fitted(fullmodel)
fitmodel <- lm(waugh_clean$price~fitted)
summary(fitmodel)
```

# 4. Assessing the Stability of the Hedonic Price Equation for the First and Second-Generation Computers
### A.
### B.
Chow's model: LN_RENT= β0 + β1LN_MEM + β2LN_MULT + β3LN_ACCESS + u
My first f-test yielded the following statistics: Chow F-statistic: 0.5574, df = (20, 58), p-value = 0.9256
My second f-test yielded the following statitstics: Chow F-statistic: 0.6243, df = (20, 31), p-value = 0.863723

I also find associated p-values of 0.9256 and 0.8637 respectively. Based on these findings I fail to reject the null hypothesis, finding lack of evidence that the slopes differ. This is similar to Chow's findings.  
 

```{r}
library("readxl")
chow <- read_excel('/Users/kieran/Documents/MASTERS/METRICS/code/metrics/practicum_2_files/Practicum 2 Chow Data ARE 256A FQ 2025.xlsx')

# data cleaning and constructions
chow_clean <- chow |> 
  rename(
    obs = "Obs",
    volume = "VOLUME",
    rent = "RENT",
    binary = "BINARY",
    digits = "DIGITS",
    words = "WORDS",
    add = "ADD",
    mult = "MULT",
    access = "ACCESS",
    year = "YEAR",
    order = "ORDER",
    ibmdum = "IBMDUM"
  ) |> 
    mutate(
      ln_rent = log(rent),
      ln_mult = log(mult),
      ln_access = log(access),
      ln_add = log(add),
      mem = words*binary*digits,
      ln_mem = log(mem)
    )

#constrained data and model
constr_chow <- chow_clean |> 
  filter(year %in% c(60, 61, 62, 63, 64, 65))

# constructing clope coefficients like in 3.a.
# create cor matrix
order_chow <- c("ln_rent", "ln_mem", "ln_mult", "ln_access")
cor_matrix_chow <- cor(chow_clean)
# cor_matrix_chow <- cor_matrix[order_chow, order_chow]

# round the matrix 
cor_matrix_rounded_chow <- round(cor_matrix_chow, 5)

# lower triangle with empty strings 
cor_matrix_char_chow <- format(cor_matrix_rounded_chow, nsmall = 5)
cor_matrix_char_chow[lower.tri(cor_matrix_char_chow)] <- ""

print(cor_matrix_char_chow, quote = FALSE)

# now its time to run the constrained and unconstrained models
# Common slopes, different intercepts by year
pooled <- lm(data = constr_chow, ln_rent~factor(year)+ln_mem+ln_mult+ln_access)
rss_pooled <- sum(resid(pooled)^2)

# allowed to differ
library(broom)
by_year <- constr_chow %>%
  group_by(year) %>%
  do(tidy(lm(ln_rent ~ ln_mem + ln_mult + ln_access, data = .)))
by_year

# cell-means parameterization: per-year intercepts and per-year slopes
m_interacted_cm <- lm(ln_rent ~ factor(year) + factor(year) + factor(year), data = chow_clean)
summary(m_interacted_cm)

# fit pooled model and get its rss
pooled <- lm(ln_rent ~ factor(year) + ln_mem + ln_mult + ln_access, data = constr_chow)
RSS_pooled <- sum(resid(pooled)^2)

# run separate regressions by year and sum RSSs
years <- unique(constr_chow$year)
RSS_years <- 0
nyears <- length(years)
k <- 4  
N_total <- 0
for (yy in years) {
  sub <- subset(constr_chow, year == yy)
  fit <- lm(ln_rent ~ ln_mem + ln_mult + ln_access, data = sub)
  RSS_years <- RSS_years + sum(resid(fit)^2)
  N_total <- N_total + nrow(sub)
}

# calculate degrees of freedom
numerator_df <- k * (nyears - 1)
denominator_df <- N_total - k * nyears

# calculate Chow F-statistic
F_stat <- ((rss_pooled - RSS_years) / numerator_df) / (RSS_years / denominator_df)

# find p-value
p_value <- pf(F_stat, numerator_df, denominator_df, lower.tail = FALSE)
cat(sprintf("Chow F-statistic: %.4f, df = (%d, %d), p-value = %.4g\n", F_stat, numerator_df, denominator_df, p_value))

# now to do the same but for the years 50-59
# filter data for years 54–59
constr_chow_old <- chow_clean %>% 
  filter(year %in% c(54, 55, 56, 57, 58, 59))

# fit pooled model for those years
pooled_old <- lm(ln_rent ~ factor(year) + ln_mem + ln_mult + ln_access, data = constr_chow_old)
RSS_pooled_old <- sum(resid(pooled_old)^2)

# run separate regressions and sum RSSs for each year (from constr_chow_old)
years_old <- unique(constr_chow_old$year)
RSS_years_old <- 0
nyears_old <- length(years_old)
k_old <- 4  
N_total_old <- 0
for (yy in years_old) {
  sub_old <- subset(constr_chow_old, year == yy)
  fit_old <- lm(ln_rent ~ ln_mem + ln_mult + ln_access, data = sub_old)
  RSS_years_old <- RSS_years_old + sum(resid(fit_old)^2)
  N_total_old <- N_total_old + nrow(sub_old)
}

# calculate degrees of freedom
numerator_df_old <- k_old * (nyears_old - 1)
denominator_df_old <- N_total_old - k_old * nyears_old

# calculate Chow F-statistic
F_stat_old <- ((RSS_pooled_old - RSS_years_old) / numerator_df_old) / (RSS_years_old / denominator_df_old)
p_value_old <- pf(F_stat_old, numerator_df_old, denominator_df_old, lower.tail = FALSE)

cat(sprintf(
  "Chow F-statistic: %.4f, df = (%d, %d), p-value = %.4g\n",
  F_stat_old, numerator_df_old, denominator_df_old, p_value_old))
```

### C.
I find a f statistic of 3.6926, with a corresponding p value of 0.01389. Since the p-value shows that the results are significant at a p<0.05 level, I reject the null hypothesis. I determine that there is statistically significant evidence that the relationship between ln(rent) and one of more of the explanatory variables used in the model changed between the generations of computers. This does not really surprise me, since I would expect technological value relationships to change over time as technology becomes more capable. I think this demonstrates that a single hedonic pricing framework fails to fit both generations correctly due to external dynamics shifting between generations. Next I relax the assumption of slope parameter equality within each generation and test the null hypothesis that slope parameters are equal over the entire 1954-1965 time span against the alternative hypothesis that these slope coeﬃcients varied from year to year. I find an f-statistic of 0.7328 and a corresponding p-value of 0.8721. This means that there is no significant evidence that the relationship between features and rent changes from year to year over the entire period, leading me to fail in rejecting the null hypothesis. These findings are consistent with those found earlier since it is plausible that the relationship could only change at the generational boundary while remaining constant withinm each generation. These results point towards a large shift but not indefinite instability between sub-periods. 

```{r}
# full regression not filtering for year; restricted model
full_model <- lm(ln_rent ~ factor(year) + ln_mem + ln_mult + ln_access, data = chow_clean)
summary(full_model)

# unrestricted 
chow_clean <- chow_clean %>%
  mutate(gen = ifelse(year <= 59, "first", "second"))

model_by_gen <- lm(ln_rent ~ factor(year) + ln_mem * gen + ln_mult * gen + ln_access * gen, data = chow_clean)
summary(model_by_gen)

# test! set up equation
rss_restricted <- sum(resid(full_model)^2)
rss_unrestricted <- sum(resid(model_by_gen)^2)

numerator_df <- 3  
k_unrestricted <- length(coef(model_by_gen))  

denominator_df <- nrow(chow_clean) - k_unrestricted
F_stat <- ((rss_restricted - rss_unrestricted) / numerator_df) / (rss_unrestricted / denominator_df)
p_value <- pf(F_stat, numerator_df, denominator_df, lower.tail = FALSE)
cat(sprintf("Chow F-statistic: %.4f, df = (%d, %d), p-value = %.4g\n", F_stat, numerator_df, denominator_df, p_value))

#Now I relax the assumption of slope parameter equality within each generation and test the null hypothesis that slope parameters are equal over the entire 1954-1965 time span against the alternative hypothesis that these slope coeﬃcients varied from year to year
# run the model
pooled <- lm(ln_rent ~ factor(year) + ln_mem + ln_mult + ln_access, data = chow_clean) 
RSS_pooled <- sum(resid(pooled)^2)
# set up the test
years <- unique(chow_clean$year)
RSS_years <- 0
nyears <- length(years)
k <- 4  
N_total <- 0
for (yy in years) {
  sub <- subset(chow_clean, year == yy)
  fit <- lm(ln_rent ~ ln_mem + ln_mult + ln_access, data = sub)
  RSS_years <- RSS_years + sum(resid(fit)^2)
  N_total <- N_total + nrow(sub)
}
# calculate p value and interpret 
numerator_df <- k * (nyears - 1)
denominator_df <- N_total - k * nyears
F_stat <- ((RSS_pooled - RSS_years) / numerator_df) / (RSS_years / denominator_df)
p_value <- pf(F_stat, numerator_df, denominator_df, lower.tail = FALSE)
cat(sprintf("Chow F-statistic: %.4f, df = (%d, %d), p-value = %.4g\n", F_stat, numerator_df, denominator_df, p_value))

```

# 5. Using Time-Varying Hedonic Price Equations to Construct Chained Price Indexes for Computers
### A.
In comparing the year-to-year changes in the estimated coeﬃcients of the 11 dummy variables with the levels of the βt estimates, I can see that the estimates are relatively close to each other between methods, with directionality consistently matching. I do see the potential for higher order interactions to exist that arent captured in the smaller adjacent year regressions. This could be due to slope inconsistency or potentially omitted interactions. I think it is appropriate to compare year-to-year changes in the estimated dummy variable coeﬃcients with levels of the estimated βt because both methods provide estimates of the annual adjusted price change. THey are both expected to be similar if the model is well specified and if the sample sizes are of reasonable size. I notice some more substantial differences between the years 56, 57, and 62, which could be the result of the adjacent year models' use of only two years of data per estimate. It could similarly have to do with the pooled version using data from all of the years which could effectively smooth away more volatility per year. I think the differences could be the result of either actual heterogeneity being picked up by the models, or some sort of misspecification due to the nature of the approaches taken. 
```{r}
# create lists to store results
adjacent_betas <- numeric()
years <- 54:64 # last pair is 64-65

for (yy in years) {
  # filter data for the adjacent years
  dat_pair <- chow_clean %>%
    filter(year %in% c(yy, yy + 1)) %>%
    mutate(dummy = ifelse(year == (yy + 1), 1, 0))

  # run regression
  fit <- lm(ln_rent ~ dummy + ln_mem + ln_mult + ln_access, data = dat_pair)
  # extract beta for the adjacent year dummy (beta t)
  beta_t <- coef(fit)["dummy"] # get coefficient by name
  adjacent_betas <- c(adjacent_betas, beta_t)
}

# year names for each beta coefficient
names(adjacent_betas) <- paste0('beta_', as.character(55:65))

# print out the adjacent year beta estimates
print(adjacent_betas)

# now for the traditional hedonic approach on the 11 dummies and other variables
hedonic_time_dummy <- lm(ln_rent ~ factor(year) + ln_mem + ln_mult + ln_access, data = chow_clean)
summary(hedonic_time_dummy)
```




