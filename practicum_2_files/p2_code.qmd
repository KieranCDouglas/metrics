---
title: "Practicum 2"
format:
  pdf:
    code-block-font-size: 0.5em
editor: visual
author: "Kieran Douglas"
---
# 2. Examining Waugh’s 1927 Asparagus Data
### A.
We can see that the greatest difference between coeff estimates and those reported in the question are in the number of stalks and the variation in size (nostalks and disp). These end up being an absolute difference of 0.1767 and 0.0697 respecitvely. 

```{r}
# Set up environment and load data
library(tidyverse)
library(readr)
library(fixest)

waugh <- read_table("/Users/kieran/Documents/MASTERS/METRICS/code/metrics/practicum_2_files/waugh.txt")
waugh_clean <- waugh |> 
  rename(
    green = "GREEN",
    nostalks = "NOSTALKS",
    disp = "DISPERSE",
    price = "PRICE"
  ) |> 
   mutate(
      green = as.numeric(green),
      nostalks = as.numeric(nostalks),
      disp = as.numeric(disp),
      price = as.numeric(price)
    )

# Run a MLR
model1 = lm(price ~ green+nostalks+disp, data = waugh_clean)
summary(model1)

# compare coefficient estimates
coeffs = coefficients(model1)
original_coeffs = c(green = 0.13826, nostalks = -1.53394, disp = -0.27554)
differences <- coeffs[names(original_coeffs)] - original_coeffs
print(differences)
# We can see that the greatest difference between coeff estimates and those reported in the question are in the number of stalks and the variation in size (nostalks and disp). These end up being an absolute difference of 0.1767 and 0.0697 respecitvely. 
```

### B. 
It looks like the main issue here is that Waugh has transformed green color on the asparagus variable to inches rather than what I have in the raw data, being hundredths of inches. To fix this I need to rescale the variable green by dividing it by 100. After doing this the difference in means is no longer a problem. 
```{r}
means <- c(price_avg = mean(waugh_clean$price), green_avg = mean(waugh_clean$green), nostalks_avg = mean(waugh_clean$nostalks), disp_avg = mean(waugh_clean$disp))
print(means)
means_reported <- c(price_avg = 90.095, green_avg = 5.8875, nostalks_avg = 19.555, disp_avg = 14.875)
differences_avg <- means[names(means_reported)] - means_reported
print(differences_avg)
# It looks like the main issue here is that Waugh has transformed green color on the asparagus variable to inches rather than what I have in the raw data, being hundredths of inches. To fix this I need to rescale the variable green by dividing it by 100. 
waugh_clean <- waugh_clean |> 
  mutate(
    green = green/100
  )
# Recheck mean differences
means_recheck <- c(price_avg = mean(waugh_clean$price), green_avg = mean(waugh_clean$green), nostalks_avg = mean(waugh_clean$nostalks), disp_avg = mean(waugh_clean$disp))
print(means_recheck)

```

### C.
I notice relative variance in the relative size of the covariances reported. For example, the covariance between price and green according to Waugh is ~3430 while the one I found was 3448 (larger). On the other hand, Waugh found a covairance of ~-154 for green and disp while I found one of ~-180 (mine was smaller). In other cases though mine was larger like with the cov between green and price. I think that the pattern in differences may relate to the way my price variable is coded relative to Waugh's. This would explain the constant differences in price covariance versus the more consistent findings in ohter categories. 
```{r}
# I am now going to create a variance covariance matrix similar to Waugh's`
moments_subset <- waugh_clean[, c("price", "green", "nostalks", "disp")]
# to match the layout of the table provided I will modify green to be back to hundredths of an inch.
moments_subset <- moments_subset |> 
  mutate(
    green = green*100
  )
# Create the matric, format it correctly, and print
matrix <- cov(moments_subset)
print(matrix)
matrix_fmt <- formatC(matrix, format="f", digits=1)
matrix_fmt[lower.tri(matrix_fmt)] <- ""
print(noquote(matrix_fmt))
```

### D.
It is plausible that the principle findings concerning the effects of covariate variation could be affected in terms of the precision and statistical inference of coefficient estimates if there was underestimation or misreporting of variance and covariance among regressors. Depsite OLS estimates remaining unbiased under GM, Waugh's under or over-estiamted variance and coveriance could lead to underestiated standard errors and possibly inacurate significance. Similarly, cov among the regressors included may inflate the standard errors and lead to reductions in the validity/reliability of estimates. The difference in scaled coefficients between Waugh's and my own analysis do not differ too much, but there are some clear differences. For example, my estimate for the marginal effect of a unit increase in the number of stalks per bunch is about $0.5 less than Waugh's. I estimate a slightly higher increase in price given a unit increase in green, and a roughly $0.2 larger decrease in price given a unit increase in size variation. Overall, our estimates are very similar. I find the association between both green and nostalks with price to be highly statistically significant at the p<0.001 level and the association between disp and price to be statistically signifiacnt at **0.001<p<0.01** level. All of these observations are ceterus parabus. 

 ```{r}
  # run model
coef(model1) * 2.782
waugh_scaled <- original_coeffs*2.782
print(waugh_scaled)
 
# The difference in scaled coefficients between Waugh's and my own analysis do not differ too much, but there are some clear differences. For example, my estimate for the marginal effect of a unit increase in the number of stalks per bunch is about $0.5 less than Waugh's. I estimate a slightly higher increase in price given a unit increase in green, and a roughly $0.2 larger decrease in price given a unit increase in size variation. Overall, our estimates are very similar. I find the association between both green and nostalks with price to be highly statistically significant at the p<0.001 level and the association between disp and price to be statistically signifiacnt at 0.001<p<0.01 level. All of these observations are ceterus parabus. 
 ```

### E. 
To solve this I am going to experiment with a matrix. 
``` {r}
# my matrix solutions
# Extract covariance matrix of regressors (columns and rows 2 to 4)
xx <- matrix[2:4, 2:4]
# Extract covariance vector of PRICE with regressors (row 1, columns 2 to 4)
xy <- matrix[1, 2:4]
# Compute beta_hat = solve(Sigma_XX) %*% Sigma_XY
beta_hat <- solve(xx) %*% xy
print(beta_hat)

# Waugh's matrix solutions
vc_mat <- matrix(c(
  1063.64, 3430.89, -100.92, -82.35,
  3430.89, 24317.19, -17.01, -154.54,
  -100.92, -17.01, 61.33, 25.51,
  -82.35, -154.54, 25.51, 83.07
), nrow = 4, byrow = TRUE)
# names for reference
row_col_names <- c("price","green","nostalks","disp")
dimnames(vc_mat) <- list(row_col_names, row_col_names)
# price-cov covariance vector
Sigma_yX <- vc_mat["price", c("green", "nostalks", "disp")]
# covariate-covariance matrix
Sigma_XX <- vc_mat[c("green", "nostalks", "disp"), c("green", "nostalks", "disp")]
# raw coefficients from matrix
beta_raw <- solve(Sigma_XX, Sigma_yX)
names(beta_raw) <- c("green", "nostalks", "disperse")
# scale
beta_final <- beta_raw * 2.782
beta_final
# Computing the least squares estimates from Waugh's VC matrix table, we can see that his estimates are fairly similar in absolute value to mine. The differences in findings may come down to several factors, including that perhaps due to the nature of OLS and our estimators being assumed to be BLUE, we have better linear unbiased estimators than Waugh was able to find. It is possible that he could have faced some rounding errors here and there that I did not, both in the calculation of his var-cov matrix and his coefficients. 
```

# 3. Exploring Relationships among $R^2$, Coeﬃcients of Determination, and Correlation Coeﬃcients
### A.
Based on the cor matrix, it is evident that the most highly correlated variables are price and green, price and nostalks, nostalks and disp, and price and disp (ordered from highest to lowest positive and negative correlation). The most orthogonal correlations are between green and nostalks and green and disp becasue each of their correlations is quite close to zero, thus implying low to no real linear relationship between the variables based on the data we have available. 
```{r}
# create cor matrix
order <- c("price", "green", "nostalks", "disp")
cor_matrix <- cor(waugh_clean)
cor_matrix <- cor_matrix[order, order]

# round the matrix 
cor_matrix_rounded <- round(cor_matrix, 5)

# lower triangle with empty strings 
cor_matrix_char <- format(cor_matrix_rounded, nsmall = 5)
cor_matrix_char[lower.tri(cor_matrix_char)] <- ""

print(cor_matrix_char, quote = FALSE)
# based on the cor matrix, it is evident that the most highly correlated variables are price and green, price and nostalks, nostalks and disp, and price and disp (ordered from highest to lowest positive and negative correlation). 
```

### B.
Comparing the square roots calculated of the R^2 I can see that they are equal except for sign. This is because the correlation coefficient can be negative (between -1 and 1) but the coiefficient of determination will always be positive (between 0 and 1) since its the squared correlation coefficient. If I had run the reverse regressios the R^2 measures would be the same as those from the correct regressions because when dealing with the same two variables, the ammount of variation explained in one variable by the other will be constant since again, it is the squares correlation coefficient. Reversing their order has no impact on the ammount of variation explained that our R^2 captures. 
```{r}
# price model
pricegreenmodel <- lm(data = waugh_clean, price ~ green)
pricenostalksmodel <- lm(data = waugh_clean, price ~ nostalks)
pricedispmodel <- lm(data = waugh_clean, price ~ disp)
#price on green 
summary(pricegreenmodel)
sqrt(0.56)
# price on nostalks
summary(pricenostalksmodel)
sqrt(0.1653)
# price on disp
summary(pricedispmodel)
sqrt(0.1054)
```

### C.
I expect that if I add the regressor nostalks to the regression, the R^2 will increase. This is because we have seperately confirmed that variation in nostalks explains some of the variation in price, so if we add it to the regression, the models explanatory power should increase. Given the correlation between green and nostalks in the table I would expect the change in R^2 when nostalks is added to the regression to be large. My intuition is such that since the two variables are not super correlated BUT are indidivually correlated with price, they should explain a significant ammount of the variation in price when both included as regressors. Running the regression confirms my intuition, as the R^2 is increased by nearly 15 points, representing a significant jump. Further, comparing the R^2 between the model that regressed price on green to the model that regressed price on green AND disp, we can see another increase in the R^2 equating to about 5 points. This represents an interesting jump, despite it being smaller in magnitude to the previous change in R^2. I think this is consistent with the sample correlation between green and disp, which is bigger than that between green and nostalks but still not so big as to fail in adding additional explanation to the model. Finally, when comparing the price on nostalks model to the price on nostalks AND disp model, we also observe a jump in R^2 that is about 4 points. This is again smaller than previous jumps and can probably be explained by the higher correlation coefficient between nostalks and disp that comes out to 0.35003, an R that would suggest higher correlation between the two explanatory variables and thus a smaller increase to the R^2 when both included in a model explaining price. This is consistent with the previous findings regarding the effect of adding correlated explanatory variables to a model. I think this is why would prioritize using adjusted R^2 since that measure penalizes for adding less relavent variables. 
```{r}
green_nostalks_model <- lm(data = waugh_clean, price~green+nostalks)
summary(green_nostalks_model)
summary(pricegreenmodel)

green_disp_model <- lm(data = waugh_clean, price~green+disp)
summary(green_disp_model)
summary(pricegreenmodel)

nostalks_disp_model <- lm(data = waugh_clean, price~nostalks+disp)
summary(nostalks_disp_model)
summary(pricenostalksmodel)
```

### D.
Here I notice the same trend, where the R^2 for the full regression is 0.7268 while the summed R^2 from the single regressions is 0.8307. I think this is because in this case, the model accounts for overlaps in explanatory power between the regressors that are not even a question in the single models. I think this is becasue the multiple regression avoids double counting, providing the joint effect on price. 
```{r}
fullmodel <- lm(data = waugh_clean, price~nostalks+green+disp)
summary(fullmodel)
# R^2 of 0.7268
0.56+0.1653+0.1054
# sum of individual R^2 = 0.8307

```

### E. 
Waugh's interpretation of the coefficient of determination is incorrect. His method of adding up individual coeficients of determination is not the right way to do it, as we know the explanatory variables he is including are not orthogonal. Waugh should have first correctly calculated and second explained that the coefficient of determination is the portion of variance in price that is jointly explained by variance in the explanatory variables, and in the case of this model, it is equal to about 0.72 or 72%. 
```{r}
# nostalks
numerator1 <- cov(waugh_clean$nostalks, waugh_clean$price) * (nrow(waugh_clean) - 1)
denominator1 <- var(waugh_clean$price) * (nrow(waugh_clean) - 1)
ratio1 <- numerator1 / denominator1
ratio1*-1.3573
# green
numerator2 <- cov(waugh_clean$green, waugh_clean$price) * (nrow(waugh_clean) - 1)
denominator2 <- var(waugh_clean$price) * (nrow(waugh_clean) - 1)
ratio2 <- numerator2 / denominator2
ratio2*13.7598
# disp
numerator3 <- cov(waugh_clean$disp, waugh_clean$price) * (nrow(waugh_clean) - 1)
denominator3 <- var(waugh_clean$price) * (nrow(waugh_clean) - 1)
ratio3 <- numerator3 / denominator3
ratio3*-0.3453

```

### F.
Right off the bat I notice that the R^2 calculated are the same! I think this happened because by regressing price on a vector of fitted values that were fitted on a regression on price, I am effectively decomposing price to simply its predicted and residual parts. As for the intercept coefficient of 0 and the slope coefficient of 1, I think this is due to the fact that the vector of fitted values is a linear transformation of the original regressors, meaning that when I go onto regress price on that vector, the best linear fit for it would be a zero intercept and a 1 slope. This is because the fitted values generated are the closest possible linear predictors of the observed data. 

```{r}
fullmodel <- lm(data = waugh_clean, price~nostalks+green+disp)
summary(fullmodel)
fitted <- fitted(fullmodel)
fitmodel <- lm(waugh_clean$price~fitted)
summary(fitmodel)
```

# 4. Assessing the Stability of the Hedonic Price Equation for the First and Second-Generation Computers
### A.

```{r}

```



