---
title: "Practicum 3"
format: pdf
author: Kieran Douglas
date: 12-02-2025
---

# 2. Replicating Nerlove's Classic Results on Scale Economies

## A.

The purpose of this assignment is to replicate some of the principles of returns-to-scale reported by Nerlove in his 1955 article. His estimated equation was as follows:

$lnC^* = \beta_o + \beta_{y}ln(y)+\beta_1lnp^*_1+\beta_2lnp^*_2$

For part A I will generate the variables necesary for estimating his parameters. includes $lnCP 3 = ln(COSTS/PF), lnP13 = ln(PL/PF), lnP23=ln(PK/PF), lnKWH=ln(KWH)$

I will do this using the mutate function in Tidyverse. 
```{r}
# Load packages and data from the NERLOV file 
library(tidyverse) 
library(readxl)

nerlov <- read_excel("/Users/kieran/Documents/MASTERS/METRICS/code/metrics/practicum_3_files/nerlov.xlsx")

# Clean data and generate new variables per assignment request

clean <- nerlov |> 
    mutate(
        LNCP3 = log(COSTS/PF), 
        LNP13 = log(PL/PF),
        LNP23 = log(PK/PF), 
        LNKWH = log(KWH)
        )

# Preview order of LNKWH

print(clean$LNKWH) 
# The observations are by size of output, looks good!
```

## B.
Now that I have all of the data in line, I will estimate the following model:

$lnC^* = \beta_o + \beta_{y}ln(y)+\beta_1lnp^*_1+\beta_2lnp^*_2$
 
Running this model I find coefficient estimates that slightly differ from those found by Nerlove, with $beta_y$, $beta_1$ and $beta_2$ estimates coming in at $0.7207$, $0.5929$, and $-0.0074$, respectively, with standard errors reporting as $0.0174$, $0.2046$, and $0.1907$. 
```{r}
model1 <- lm(data = clean, LNCP3~LNKWH+LNP13+LNP23)
summary(model1)
```

## C.
I will now construct a confidence interval for $beta_y$. This ends up being ($0.6751603 0.7662147$), which means that using the same procedure on repeated samples, we would expect 99% of intervals to contain the true population coefficient. Based on this, we could determine that at the 99% confidence interval, the null hypothesis that $beta_y=1$ is rejected, meaning that with a high degree of certainty we could say that values of 1 are not expected as the true population coefficient under this model. This implies that we would reject the null hypothesis that returns to scale were constant, because we can say with a high degree of certainty that they will be increasing since our confidence interval shows that we would expect 99% of intervals (0.68, 0.77) to contain the true population coefficient. If the interval contained 1 we may say that it is plausible for the model to display constant returns to scale, but since it is <1, we can say with a reasonable degree of confidence that in most all cases a 1% increase in the output will raise cost by less than 1% which implies increasing returns to scale. Computing the point estimate for returns to scale, I find an r of 1.388. Since this is greater than 1, we again confirm observation of increasing returns to scale. This means that if we werer to double inputs, outputs would more than double! Since returns to scale are increasing, in this case we have positive economies of scale. This means that as output increases, cost increases at a decreasing rate, or avarage cost falls. 

```{r}
# Construct a 99% confidence interval using the estimated model for the coefficient on LNKWH ($beta_y$)
confint(model1, "LNKWH", level = 0.99)

# compute the point estimate of returns to scale r, where r=1/b_y. It is 1.388
1/0.720688
```

## D.
To calculate the implies estimate of $alpha_2$ from part C, I have to multiply the coefficient $beta_2$ by the point estimate for returns to scale, r. This yields $-0.007381*1.388=-0.01024$. This estimate is very close to 0, which makes me think that Nerlove was dissatisfied with his estimate of $alpha_2$ since it would imply that the second input (capital) was either not effective or negatively productive which wouldnt really make sense for a necessary input. Additionally, since the estimate is so close to 0, it seems likely hat a confidence interval would contain 0 deeming it non statistically significant which also wouldnt make sense. This finding contradicts what we would expect of Cobb-Douglas, indicating that something weird might be going on with the data we have that is causing it to inaccurately reflect the true production process.   

## E.
I also find a U-shaped pattern when plotting residuals against LNKWH. I think that this could be a sign of model mispsecification, where the model we are estimating does not adequetly capture the underlying relationship in the data. The implication here is that the true relationship between output and cost is likely nonlinear in some way, and our linearity asusmption is failing. Next, calculating the correlation between the residuals and LNKWH I find a coefficient of effectively 0 (-9.565708e-17). This is not surprising as OLS ensures that residuals are not correlated with regressors in the model by optimizing the fit such that linear association between residuals and regressors is removed. This however is not indicitive of a perfect model, and provides a good example of why its important to check things graphically to ensure that we are getting what we expected. 
```{r}
# create a new row with residuals from the model
clean <- clean |> 
    mutate(
        resid = residuals(model1)
    )

# plot against log of output
ggplot(data = clean, mapping = aes(y = resid, x = LNKWH)) +
    geom_point() +
    geom_smooth(method = loess) +
    theme_minimal() +
    labs(title = "Residuals Plotted Against Log of Output", x = "LNKWH", y = "Residuals")

# compute sample correlation of residuals with LNKWH across the sample 
cor(clean$LNKWH, clean$resid)

```

# 3. Assessing Alternative Returns-to-Scale Specifications
## A. 
My attempted replication of Nerlove's results was actually far closer than I was expecting. Both the estimates and the standard errors were within a very small distance of each other, and any spread appeared to be relatively unsystematic, varying from being greater than or less than with within and between subsets. I would estimate that any observed discrepencies are probably due to the fact that I used natural logarithms in my transformations while in the reported data from Nerlove common logarithms were used. 
```{r}
# generate subsamples of data for each set of 29 rows
ss1 <- clean |> 
    filter(
        ORDER <200
    )
ss2 <- clean |> 
    filter(
        ORDER > 200,
        ORDER < 300
    )
ss3 <- clean |> 
    filter(
        ORDER > 300,
        ORDER < 400
    )
ss4 <- clean |> 
    filter(
        ORDER >400,
        ORDER <500
    )
ss5 <- clean |> 
    filter(
        ORDER > 500
    )
# now I will estimate a model for each of these subsets and extract the coefficients to combine in a table
library(broom)

ss1_model <- lm(data = ss1, LNCP3~LNKWH+LNP13+LNP23)
ss2_model <- lm(data = ss2, LNCP3~LNKWH+LNP13+LNP23)
ss3_model <- lm(data = ss3, LNCP3~LNKWH+LNP13+LNP23)
ss4_model <- lm(data = ss4, LNCP3~LNKWH+LNP13+LNP23)
ss5_model <- lm(data = ss5, LNCP3~LNKWH+LNP13+LNP23)

# create a conbined coefficients vector to use in table
coef_combined <- list(ss1_model, ss2_model, ss3_model, ss4_model, ss5_model)
# create and print tables for each subset
coef_tbl <- lapply(seq_along(coef_combined), function(i) {
  tidy(coef_combined[[i]]) %>%
    mutate(coef_combined = paste0("model", i))
})
combined <- bind_rows(coef_tbl)
print(coef_tbl)
```

## B.
Using the subset models I ran in the previous section, I compute point estimates of returns to scale (r=1/b_y) for each of the five subsamples. For subsets 1-5 I find point estimates of returns to scale of 2.500, 1.519, 1.066, 1.096, and 0.961 respectively. These point estimates suggest that as output increases, cost generally decreases suggesting on average positive returns to scale. There are discrepensies that I observe though, as the magnitude at which the returns to scale are increasing are decreasing as output increases. Additionally, in the final subset, we see a switch to what may be either slightly decreasing or constant returns to scale. This indicates that at larger quantities bottlenecks or constraints may be experienced that limit continued gains. I think this suggests an alternative specification that is perhaps broken up by quantity and interpreted in terms of the subsections rather than their aggregate. The implications are that returns to scale may not be consistent in this case (or any really), reflecting the U-shaped residuals plot we observed in the previous section. 

```{r}
# subsample 1
1/0.4
#subsample 2
1/0.658
# subsample 3
1/0.938
# subsample 4
1/0.912
#subsample 5
1/1.04

```

## C.
I think that since the $beta_y$ coefficients vary pretty substantially across groups but the $beta_1 and beta_2$ coefficients do not as much. Basically, the output causes pretty different cost responses but the responsiveness to different input prices remains relatively stable. Given this, I think that by allowing intercept terms and $beta_y$ coefficients to vary while holding the others constant across groups. This should allow for us to better control for the observed heteogeneity, generating a model that is a better specification of our sample given its observed shape in earlier parts. After replicating Nerlove's model by including all of the dummies and their interactions with $beta_y$ I was able to achieve almost a perfect copy of his estimates. Comparing mine to Nerlove's, the ones that are the most different are out estimated coefficients on the interaction between the second dummy (those between 200 and 300) in which he estimated 0.651 while I estimated 0.648, and our $R^2$ which he estimated as 0.95 while I found it to be 0.96. As you can see, these are VERY close to one another and any differences probably have to do again with the fact that I used natural logarithms in transformation while Nerlove used common logarithms. I believe that this also explains why all of my standard errors are slightly smaller than those reported by Nerlove.

```{r}
# I am going to start by creating dummy variables representing each group, followed by variables that multiply that dummy by LNKWH to act as propper interaction terms and finally rerun the regression including our new variables. 
# the dummies are called d1-d5
clean <- clean |> 
    mutate(
        d1 = ORDER <200,
        d2 = ORDER>=200 & ORDER<300,
        d3 = ORDER>=300 & ORDER<400,
        d4 = ORDER>=400 & ORDER<500,
        d5 = ORDER>=500,
        d1_LNKWH = d1*LNKWH,
        d2_LNKWH = d2*LNKWH,
        d3_LNKWH = d3*LNKWH,
        d4_LNKWH = d4*LNKWH,
        d5_LNKWH = d5*LNKWH
    )
# rerun the model including teh dummy variables
model2 <- lm(data = clean, LNCP3~d1+d2+d3+d4+d5+d1_LNKWH+d2_LNKWH+d3_LNKWH+d4_LNKWH+d5_LNKWH+LNP13+LNP23)
summary(model2)

```

## D.
Using the model I estimated in the previous section, I will now use the point estimate returns to scale formula from part B. to compute the implied estimate of returns to scale for each of the five subsamples. From subsamples 1-5 I find point estimate returns to scale of 2.52, 1.54, 1.13, 1.10, and 0.94 respectively. Similar to my findings in part B. we observe increasing returns to scale that decrease with each subsequent output increase. THis trend continues until we reach subsample 5 which exhibits decreasing (perhaps constand) returns to scale, which seems to be a common trend as output scales up due to physical constraints. Basicallu, my estimated scale economies are quite strong when output is very small, but as output grows the benefits diminish. Once we weach the highest reported output point we even notice constand or decreasing average cost per unit meanign that further growth is no longer bringing an advantage as it once did.  
```{r}
# subsample 1
1/0.39688
#subsample 2
1/0.64816
# subsample 3
1/0.88479
# subsample 4
1/0.90874
#subsample 5
1/1.06274
```

## E. 

```{r}

```



